{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Tokenizer Training",
   "id": "f13cb3a3f236e33a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-19T07:31:29.327514Z",
     "start_time": "2024-12-19T07:31:22.557354Z"
    }
   },
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "import os\n",
    "import json\n",
    "from transformers import BertTokenizer, AutoModel, AutoTokenizer\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "# Step 0: Hyperparameters\n",
    "vocab_size = 30_000\n",
    "min_frequency = 2\n",
    "\n",
    "# Step 1: Download and load the dataset\n",
    "print(\"Downloading dataset...\")\n",
    "path = kagglehub.dataset_download(\"rmisra/news-category-dataset\")\n",
    "dataset_path = f\"{path}/News_Category_Dataset_v3.json\"\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Preprocess the dataset\n",
    "data_df = pd.DataFrame(raw_data)\n",
    "# Filter to a specific category\n",
    "data_df = data_df[data_df[\"category\"] == \"POLITICS\"]\n",
    "# data_df = data_df.sample(50000, random_state=42)\n",
    "text_data = data_df[\"headline\"].tolist() + data_df[\"short_description\"].tolist()\n",
    "\n",
    "print(f\"Number of samples: {len(text_data)}\")\n",
    "\n",
    "# Save text data for tokenizer training\n",
    "text_file_path = \"tokenizer_training_corpus.txt\"\n",
    "with open(text_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for text in text_data:\n",
    "        f.write(f\"{text}\\n\")\n",
    "\n",
    "# Step 2: Train the tokenizer\n",
    "print(\"Training tokenizer...\")\n",
    "pretrained_model = \"bert-large-uncased\"  # Replace with your desired model\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "\n",
    "def dataset_iterator(data):\n",
    "    for line in data:\n",
    "        yield line\n",
    "\n",
    "new_tokenizer = tokenizer.train_new_from_iterator(\n",
    "    dataset_iterator(text_data),\n",
    "    vocab_size=vocab_size\n",
    ")\n",
    "\n",
    "# Save tokenizer files\n",
    "tokenizer_dir = \"archive/custom_tokenizer\"\n",
    "os.makedirs(tokenizer_dir, exist_ok=True)\n",
    "new_tokenizer.save_pretrained(tokenizer_dir)\n",
    "\n",
    "# Save config.json for BertTokenizer\n",
    "config = {\n",
    "    \"model_type\": \"bert\",\n",
    "    \"tokenizer_class\": \"BertTokenizer\",\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"do_lower_case\": True,\n",
    "}\n",
    "with open(os.path.join(tokenizer_dir, \"config.json\"), \"w\") as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "\n",
    "# Generate vocab.txt for BertTokenizer\n",
    "vocab_path = os.path.join(tokenizer_dir, \"vocab.txt\")\n",
    "with open(vocab_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    for token, index in sorted(vocab.items(), key=lambda x: x[1]):\n",
    "        f.write(f\"{token}\\n\")\n",
    "\n",
    "# Step 3: Load the tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "try:\n",
    "    tokenizer = BertTokenizer.from_pretrained(tokenizer_dir)\n",
    "    print(\"Tokenizer loaded successfully: \", tokenizer)\n",
    "except Exception as e:\n",
    "    print(\"Error loading tokenizer:\", e)\n",
    "\n",
    "# Step 4: Load SBERT model\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/bert-base-nli-mean-tokens\")\n",
    "print(\"Model loaded successfully.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n",
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.5)\n",
      "Loading dataset...\n",
      "Number of samples: 71204\n",
      "Training tokenizer...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "001bbc3786594ae09162481f8b41975a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\global_envs\\pytorch_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Stefan\\.cache\\huggingface\\hub\\models--bert-large-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "78f477e019154080952a4337e5a8fc59"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f40c100456514170a9b8ba9865fd61a1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "049c9297b2e64869bd28951a48806c2d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Tokenizer loaded successfully:  BertTokenizer(name_or_path='archive/custom_tokenizer', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t4: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "Model loaded successfully.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Tokenizer Coverage",
   "id": "eab693c9e82852e4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T07:31:56.705910Z",
     "start_time": "2024-12-19T07:31:39.032322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Tokenize the dataset\n",
    "all_text = \" \".join(text_data)\n",
    "tokenized_output = tokenizer.tokenize(all_text)\n",
    "\n",
    "# Check unique tokens\n",
    "unique_tokens = Counter(tokenized_output)\n",
    "print(f\"Number of unique tokens: {len(unique_tokens)}\")\n",
    "\n",
    "# Compare coverage\n",
    "total_words = len(all_text.split())\n",
    "coverage = len(tokenized_output) / total_words\n",
    "print(f\"Tokenizer coverage: {coverage:.2f}\")"
   ],
   "id": "a12c19c3de64d3b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 19600\n",
      "Tokenizer coverage: 1.29\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Sample Tokenization",
   "id": "24d6322ac2d2fd22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T07:32:06.106429Z",
     "start_time": "2024-12-19T07:32:06.099794Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_texts = text_data[:5]\n",
    "for text in sample_texts:\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Tokens: {tokens}\")"
   ],
   "id": "b882aa4a517b2585",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Biden Says U.S. Forces Would Defend Taiwan If China Invaded\n",
      "Tokens: ['bid', '##en', 'says', 'u', '.', 's', '.', 'forces', 'would', 'defend', 'taiwan', 'if', 'china', 'invaded']\n",
      "Original: ‘Beautiful And Sad At The Same Time’: Ukrainian Cultural Festival Takes On A Deeper Meaning This Year\n",
      "Tokens: ['‘', 'beautiful', 'and', 'sad', 'at', 'the', 'same', 'time', '’', ':', 'ukrainian', 'cultural', 'festival', 'takes', 'on', 'a', 'deeper', 'meaning', 'this', 'year']\n",
      "Original: Biden Says Queen's Death Left 'Giant Hole' For Royal Family\n",
      "Tokens: ['bid', '##en', 'says', 'queen', \"'\", 's', 'death', 'left', \"'\", 'giant', 'hole', \"'\", 'for', 'royal', 'family']\n",
      "Original: Bill To Help Afghans Who Escaped Taliban Faces Long Odds In The Senate\n",
      "Tokens: ['bill', 'to', 'help', 'afghan', '##s', 'who', 'escaped', 'taliban', 'faces', 'long', 'odds', 'in', 'the', 'senate']\n",
      "Original: Mark Meadows Complies With Justice Dept. Subpoena: Report\n",
      "Tokens: ['mark', 'meadows', 'com', '##pl', '##ies', 'with', 'justice', 'dept', '.', 'sub', '##po', '##ena', ':', 'report']\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Sentence-BERT Tokenizer",
   "id": "396e4838ab12fe54"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T07:45:47.225028Z",
     "start_time": "2024-12-19T07:45:43.894180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load Sentence-BERT and custom tokenizer\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_dir)\n",
    "\n",
    "# Example input using the custom tokenizer\n",
    "text = \"Donald Trump recently visited the UK.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "\n",
    "decoded_text = tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n",
    "print(decoded_text)\n"
   ],
   "id": "8b202c4e19686b96",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[   2, 6221, 8398, 3728, 4716, 1996, 2866, 1012,    3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "donald trump recently visited the uk .\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
