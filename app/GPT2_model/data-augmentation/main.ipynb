{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Data Augmentation",
   "id": "7b03a419b75f532a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T22:43:48.585398Z",
     "start_time": "2024-12-18T22:43:48.579026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import random\n",
    "import torch\n",
    "\n",
    "import kagglehub\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from transformers import pipeline, GPT2Tokenizer, GPT2LMHeadModel\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import download"
   ],
   "id": "b1259b6b6fc08b2e",
   "outputs": [],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Setting up GPU",
   "id": "38d718602e157347"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T22:43:48.636797Z",
     "start_time": "2024-12-18T22:43:48.628862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Is CUDA available: \", torch.cuda.is_available())\n",
    "print(\"Number of GPUs: \", torch.cuda.device_count())\n",
    "print(\"Current device: \", torch.cuda.current_device())\n",
    "print(\"Device name: \", torch.cuda.get_device_name())\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "93caaa667226cbef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available:  True\n",
      "Number of GPUs:  1\n",
      "Current device:  0\n",
      "Device name:  NVIDIA GeForce GTX 1080 Ti\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Load the dataset",
   "id": "2b682333758da669"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T22:43:50.637286Z",
     "start_time": "2024-12-18T22:43:48.646487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the dataset\n",
    "path = kagglehub.dataset_download(\"rmisra/news-category-dataset\")\n",
    "dataset_path = f\"{path}/News_Category_Dataset_v3.json\"\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "print(\"Loading dataset...\")\n",
    "with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = [json.loads(line) for line in f]\n",
    "\n",
    "# Convert raw data into a DataFrame\n",
    "data_df = pd.DataFrame(raw_data)"
   ],
   "id": "778d60e4d5d4ae7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.5)\n",
      "Loading dataset...\n"
     ]
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "|#### Filter categories and sample the dataset",
   "id": "eb6a7a4b1c885e83"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-18T22:43:50.727927Z",
     "start_time": "2024-12-18T22:43:50.669764Z"
    }
   },
   "source": [
    "# Filter categories\n",
    "# categories = [\n",
    "#     \"POLITICS\", \"WELLNESS\", \"ENTERTAINMENT\", \"TRAVEL\", \"STYLE & BEAUTY\", \"PARENTING\",\n",
    "#     \"HEALTHY LIVING\", \"QUEER VOICES\", \"FOOD & DRINK\", \"BUSINESS\", \"COMEDY\", \"SPORTS\",\n",
    "#     \"BLACK VOICES\", \"HOME & LIVING\", \"PARENTS\"\n",
    "# ]\n",
    "categories = [\n",
    "    \"POLITICS\"\n",
    "]\n",
    "filtered_df = data_df[data_df[\"category\"].isin(categories)]\n",
    "\n",
    "# Remove rows with empty descriptions\n",
    "filtered_df = filtered_df[filtered_df[\"short_description\"] != \"\"]\n",
    "\n",
    "# Step 1: Limit dataset to a percentage\n",
    "def sample_dataset(df, percentage):\n",
    "    return df.sample(frac=percentage, random_state=42)\n",
    "\n",
    "# Adjust the percentage here\n",
    "percentage = 0.002\n",
    "sampled_df = sample_dataset(filtered_df, percentage)\n",
    "\n",
    "print(f\"Original dataset size: {len(filtered_df)}\")\n",
    "print(f\"Sampled dataset size: {len(sampled_df)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 32441\n",
      "Sampled dataset size: 65\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Data augmentation using GPT-2 and word replacements",
   "id": "41b67515226981e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T22:45:22.299209Z",
     "start_time": "2024-12-18T22:43:50.739061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Ensure WordNet is downloaded\n",
    "download(\"wordnet\")\n",
    "\n",
    "# Load GPT-2\n",
    "print(\"Loading GPT-2 model...\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Synonym replacement function using WordNet\n",
    "def replace_with_synonyms(text, num_replacements=3):\n",
    "    words = text.split()\n",
    "    for _ in range(num_replacements):\n",
    "        idx = random.randint(0, len(words) - 1)\n",
    "        word = words[idx]\n",
    "        synonyms = [lemma.name() for syn in wordnet.synsets(word) for lemma in syn.lemmas() if lemma.name() != word]\n",
    "        if synonyms:\n",
    "            words[idx] = random.choice(synonyms)\n",
    "    return \" \".join(words)\n",
    "\n",
    "# Augmentation function\n",
    "def augment_instance(instance, num_augmentations=1):\n",
    "    augmented_instances = []\n",
    "    for _ in range(num_augmentations):\n",
    "        # Construct GPT-2 prompt\n",
    "        # prompt = f\"Link: {instance['link']}\\nHeadline: {instance['headline']}\\nCategory: {instance['category']}\\nAuthors: {instance['authors']}\\nDate: {instance['date']}\\nDescription: {instance['short_description']}\\n\"\n",
    "        prompt = f\"Headline: {instance['headline']}\\nCategory: {instance['category']}\\nAuthors: {instance['authors']}\\nDate: {instance['date']}\\nDescription: {instance['short_description']}\\n\"\n",
    "\n",
    "        # Generate new text using GPT-2\n",
    "        generated_text = generator(prompt, max_length=150, num_return_sequences=1)[0][\"generated_text\"]\n",
    "\n",
    "        # Extract the generated description\n",
    "        new_description = generated_text.split(\"Description:\")[-1].strip()\n",
    "\n",
    "        # Replace random words in the description with synonyms\n",
    "        new_description = replace_with_synonyms(new_description)\n",
    "\n",
    "        # Create a new instance with updated description\n",
    "        new_instance = instance.copy()\n",
    "        new_instance[\"short_description\"] = new_description\n",
    "        augmented_instances.append(new_instance)\n",
    "    return augmented_instances\n",
    "\n",
    "# Apply augmentation\n",
    "print(\"Applying data augmentation...\")\n",
    "augmented_data = []\n",
    "for _, row in tqdm(sampled_df.iterrows(), total=len(sampled_df)):\n",
    "    num_augmentations = random.randint(0, 2)\n",
    "    augmented_data.extend(augment_instance(row.to_dict(), num_augmentations))\n",
    "\n",
    "# Combine original and augmented data\n",
    "# final_dataset = sampled_df.to_dict(orient=\"records\") + augmented_data\n",
    "\n",
    "# Combine only augmented data\n",
    "final_dataset = augmented_data\n",
    "\n",
    "# Save augmented dataset with each instance on a single line\n",
    "output_path = \"augmented_news_category_dataset.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for instance in final_dataset:\n",
    "        json.dump(instance, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"Augmented dataset saved to {output_path}\")\n"
   ],
   "id": "c6bf7a9d285249f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPT-2 model..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Stefan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying data augmentation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/65 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  2%|▏         | 1/65 [00:01<02:03,  1.93s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "  3%|▎         | 2/65 [00:03<01:41,  1.61s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 11%|█         | 7/65 [00:04<00:29,  1.97it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 17%|█▋        | 11/65 [00:05<00:22,  2.43it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 20%|██        | 13/65 [00:08<00:30,  1.68it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 23%|██▎       | 15/65 [00:09<00:34,  1.45it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 26%|██▌       | 17/65 [00:12<00:40,  1.19it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 28%|██▊       | 18/65 [00:13<00:37,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 29%|██▉       | 19/65 [00:14<00:46,  1.02s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 31%|███       | 20/65 [00:15<00:45,  1.01s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 32%|███▏      | 21/65 [00:17<00:51,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 34%|███▍      | 22/65 [00:18<00:49,  1.16s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 37%|███▋      | 24/65 [00:20<00:45,  1.10s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 40%|████      | 26/65 [00:21<00:34,  1.12it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 43%|████▎     | 28/65 [00:23<00:30,  1.23it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 46%|████▌     | 30/65 [00:25<00:34,  1.01it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 51%|█████     | 33/65 [00:26<00:22,  1.41it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 54%|█████▍    | 35/65 [00:28<00:22,  1.35it/s]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 55%|█████▌    | 36/65 [00:31<00:31,  1.07s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 57%|█████▋    | 37/65 [00:34<00:44,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 58%|█████▊    | 38/65 [00:36<00:45,  1.69s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 60%|██████    | 39/65 [00:38<00:40,  1.54s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 62%|██████▏   | 40/65 [00:40<00:45,  1.81s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 63%|██████▎   | 41/65 [00:41<00:39,  1.64s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 66%|██████▌   | 43/65 [00:43<00:26,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 69%|██████▉   | 45/65 [00:46<00:26,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 71%|███████   | 46/65 [00:48<00:27,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 72%|███████▏  | 47/65 [00:53<00:42,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 74%|███████▍  | 48/65 [00:58<00:53,  3.13s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 77%|███████▋  | 50/65 [01:00<00:34,  2.27s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 78%|███████▊  | 51/65 [01:02<00:30,  2.19s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 82%|████████▏ | 53/65 [01:07<00:27,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 83%|████████▎ | 54/65 [01:14<00:37,  3.37s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 85%|████████▍ | 55/65 [01:21<00:40,  4.01s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 88%|████████▊ | 57/65 [01:22<00:21,  2.74s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 91%|█████████ | 59/65 [01:22<00:10,  1.76s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 92%|█████████▏| 60/65 [01:24<00:08,  1.68s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 94%|█████████▍| 61/65 [01:26<00:07,  1.78s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      " 98%|█████████▊| 64/65 [01:28<00:01,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "100%|██████████| 65/65 [01:30<00:00,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented dataset saved to augmented_news_category_dataset.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T22:45:24.814898Z",
     "start_time": "2024-12-18T22:45:22.394586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test gpt2 with a prompt\n",
    "prompt = \"Trump promises that\"\n",
    "\n",
    "generated_text = generator(prompt, max_length=150, num_return_sequences=1)[0][\"generated_text\"]\n",
    "\n",
    "print(generated_text)"
   ],
   "id": "9a1b365086f85e5c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump promises that his administration would not negotiate with Iran over its nuclear program unless the U.S. gets a final deal. One would hope that Obama's administration would find that the only problem with it is a looming government-imposed deadline to fulfill a major campaign promise. Perhaps it would return to the \"nuclear nonproliferation treaties\" (NPT), as we did in 2001. But it would be a long, dangerous journey to reach that goal despite the assurances from President Bush and Vice President Cheney that Iran must have a comprehensive program to develop a nuclear weapon.\n",
      "\n",
      "The real reason behind this has not been determined. And the problem is likely deeper than the rhetoric we see it making clear.\n",
      "\n",
      "The reason is that Washington sees a\n"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T22:45:24.841454Z",
     "start_time": "2024-12-18T22:45:24.838715Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7c512ae760060ca4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
