{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fine-Tuning GPT-2 on AG News",
   "id": "c9f4336195a6c4aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 0. Setup",
   "id": "c6636f30bd3b707b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T21:28:00.237660Z",
     "start_time": "2024-12-11T21:27:57.363948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Is CUDA available: \", torch.cuda.is_available())\n",
    "print(\"Number of GPUs: \", torch.cuda.device_count())\n",
    "print(\"Current device: \", torch.cuda.current_device())\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "51955dac3042e8eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available:  True\n",
      "Number of GPUs:  1\n",
      "Current device:  0\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. Preparing the Dataset",
   "id": "33eaafc62d6144b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1.1. Loading the AG News Dataset",
   "id": "bd456b05f86321ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T21:28:16.266094Z",
     "start_time": "2024-12-11T21:28:00.507909Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments, GPT2LMHeadModel\n",
    "\n",
    "# Load AG News dataset\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "# Check dataset structure\n",
    "print(dataset)\n",
    "print(dataset['train'][0])  # Example of the first training sample"
   ],
   "id": "29532ebc47118301",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stefan\\Documents\\GitHub\\UBB-CS-Projects\\Semestrul 5\\MIRPR\\projects-queryminds\\GPT2_model\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 120000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 7600\n",
      "    })\n",
      "})\n",
      "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\", 'label': 2}\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1.2. Preprocessing the Dataset",
   "id": "c45cd3555f683afe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T21:28:16.389802Z",
     "start_time": "2024-12-11T21:28:16.272596Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract only the text field\n",
    "def prepare_text_data(example):\n",
    "    return {\"text\": example[\"text\"]}\n",
    "\n",
    "# Apply preprocessing\n",
    "processed_dataset = dataset.map(prepare_text_data, remove_columns=dataset[\"train\"].column_names)\n",
    "\n",
    "# Check the processed dataset\n",
    "print(processed_dataset[\"train\"][0])"
   ],
   "id": "cf66d885ed8faa76",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\"}\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1.3. Limiting the Dataset Size",
   "id": "d7dd2552e736f630"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T21:28:16.492143Z",
     "start_time": "2024-12-11T21:28:16.483801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the ratio (e.g., 10%)\n",
    "limit_ratio = 0.01\n",
    "\n",
    "# Calculate the limit for the dataset\n",
    "train_limit = int(len(processed_dataset[\"train\"]) * limit_ratio)\n",
    "test_limit = int(len(processed_dataset[\"test\"]) * limit_ratio)\n",
    "\n",
    "# Select a subset of the dataset\n",
    "limited_train_dataset = processed_dataset[\"train\"].select(range(train_limit))\n",
    "limited_test_dataset = processed_dataset[\"test\"].select(range(test_limit))\n",
    "\n",
    "# Check the size of the limited dataset\n",
    "print(f\"Original train size: {len(processed_dataset['train'])}\")\n",
    "print(f\"Limited train size: {len(limited_train_dataset)}\")\n"
   ],
   "id": "66ea20ff0367ca9d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train size: 120000\n",
      "Limited train size: 1200\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 1.4. Tokenizing the Dataset",
   "id": "74ad449a26372aad"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-11T21:28:18.473606Z",
     "start_time": "2024-12-11T21:28:16.515490Z"
    }
   },
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Ensure the tokenizer has a pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Define a tokenization function\n",
    "def tokenize_and_prepare_labels(example):\n",
    "    tokens = tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()  # Labels should match input_ids for language modeling\n",
    "    return tokens\n",
    "\n",
    "# Apply tokenization to the datasets\n",
    "tokenized_train_dataset = limited_train_dataset.map(\n",
    "    tokenize_and_prepare_labels,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "tokenized_test_dataset = limited_test_dataset.map(\n",
    "    tokenize_and_prepare_labels,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "# Format the datasets for PyTorch\n",
    "tokenized_train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2. Fine-Tuning GPT-2",
   "id": "f9f1f462206dbc1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T21:36:53.660071Z",
     "start_time": "2024-12-11T21:28:18.482058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Load the GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"gpt2-english\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,  # Use a larger batch size if GPU memory allows\n",
    "    gradient_accumulation_steps=2,  # Accumulate gradients for larger effective batch size\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,  # Use mixed precision training for faster computations (requires GPU)\n",
    "    load_best_model_at_end=False,\n",
    ")\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./gpt2-english\")\n",
    "tokenizer.save_pretrained(\"./gpt2-english\")"
   ],
   "id": "638dfdfc3c059f71",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 08:29, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.789410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.763629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.757206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.757422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.760376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.763089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.803200</td>\n",
       "      <td>0.769366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.803200</td>\n",
       "      <td>0.772763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.803200</td>\n",
       "      <td>0.775875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.803200</td>\n",
       "      <td>0.777806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./gpt2-english\\\\tokenizer_config.json',\n",
       " './gpt2-english\\\\special_tokens_map.json',\n",
       " './gpt2-english\\\\vocab.json',\n",
       " './gpt2-english\\\\merges.txt',\n",
       " './gpt2-english\\\\added_tokens.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3. Evaluating the Model",
   "id": "4e90f1f55997da25"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T21:36:57.516595Z",
     "start_time": "2024-12-11T21:36:53.713349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load original GPT-2\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "original_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "original_model.eval()\n",
    "\n",
    "# Load fine-tuned GPT-2\n",
    "fine_tuned_model = GPT2LMHeadModel.from_pretrained(\"gpt2-english\")\n",
    "fine_tuned_model.eval()\n",
    "\n",
    "prompt = \"Left or right?\"\n",
    "\n",
    "# Generate with original GPT-2\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs_original = original_model.generate(\n",
    "    inputs.input_ids, max_length=50, temperature=0.7, top_p=0.9\n",
    ")\n",
    "print(\"Original GPT-2:\", tokenizer.decode(outputs_original[0], skip_special_tokens=True))\n",
    "\n",
    "# Generate with fine-tuned GPT-2\n",
    "outputs_fine_tuned = fine_tuned_model.generate(\n",
    "    inputs.input_ids, max_length=50, temperature=0.7, top_p=0.9\n",
    ")\n",
    "print(\"Fine-tuned GPT-2:\", tokenizer.decode(outputs_fine_tuned[0], skip_special_tokens=True))\n"
   ],
   "id": "18b4394b1e547222",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stefan\\Documents\\GitHub\\UBB-CS-Projects\\Semestrul 5\\MIRPR\\projects-queryminds\\GPT2_model\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Stefan\\Documents\\GitHub\\UBB-CS-Projects\\Semestrul 5\\MIRPR\\projects-queryminds\\GPT2_model\\.venv\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original GPT-2: Left or right?\n",
      "\n",
      "The answer is that the right is the most important thing. It is the most important thing to you. It is the most important thing to your family. It is the most important thing to your friends. It is the\n",
      "Fine-tuned GPT-2: Left or right? The difference between the two is that the left is more likely to be a good player and the right more likely to be a bad one.\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
