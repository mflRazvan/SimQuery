{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T21:54:31.652402Z",
     "start_time": "2025-01-14T21:34:49.169319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import LoggingHandler\n",
    "import logging\n",
    "\n",
    "# Hyperparameters\n",
    "limit = 4000\n",
    "\n",
    "# GPU compatibility\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Fine-tuned model save path\n",
    "fine_tuned_model_path = 'fine_tuned_sbert_model'\n",
    "\n",
    "# Step 1: Load the STS-B dataset\n",
    "print(\"Loading the STS-B dataset...\")\n",
    "dataset = load_dataset('glue', 'stsb')\n",
    "\n",
    "# Limit the dataset size for faster training (batched processing)\n",
    "def limit_rows(batch):\n",
    "    return {\n",
    "        'sentence1': batch['sentence1'][:limit],\n",
    "        'sentence2': batch['sentence2'][:limit],\n",
    "        'label': batch['label'][:limit]\n",
    "    }\n",
    "\n",
    "# Apply the row limit to train and validation datasets\n",
    "print(f\"Limiting dataset to {limit} rows...\")\n",
    "dataset['train'] = dataset['train'].select(range(min(limit, len(dataset['train']))))\n",
    "dataset['validation'] = dataset['validation'].select(range(min(limit, len(dataset['validation']))))\n",
    "\n",
    "# Step 2: Prepare the training data\n",
    "train_sentences1 = dataset['train']['sentence1']\n",
    "train_sentences2 = dataset['train']['sentence2']\n",
    "train_scores = dataset['train']['label']\n",
    "\n",
    "# Normalize scores to the 0-1 range\n",
    "print(\"Normalizing scores...\")\n",
    "train_scores = [score / 5.0 for score in train_scores]\n",
    "\n",
    "# Prepare validation data\n",
    "val_sentences1 = dataset['validation']['sentence1']\n",
    "val_sentences2 = dataset['validation']['sentence2']\n",
    "val_scores = dataset['validation']['label']\n",
    "val_scores = [score / 5.0 for score in val_scores]\n",
    "\n",
    "# Step 3: Load the larger pre-trained SBERT model\n",
    "print(\"Loading the original SBERT model...\")\n",
    "original_model = SentenceTransformer('stsb-bert-large').to(device)\n",
    "\n",
    "# Step 4: Evaluate the original model\n",
    "print(\"Evaluating the original SBERT model...\")\n",
    "original_evaluator = EmbeddingSimilarityEvaluator(val_sentences1, val_sentences2, val_scores)\n",
    "evaluation_results = original_evaluator(original_model)\n",
    "\n",
    "# Print results\n",
    "def print_results(evaluation_results, model_name=\"SBERT model\"):\n",
    "    if isinstance(evaluation_results, dict):\n",
    "        for metric, value in evaluation_results.items():\n",
    "            print(f\"{model_name} {metric}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"{model_name} evaluation score: {evaluation_results:.4f}\")\n",
    "\n",
    "print_results(evaluation_results)\n",
    "\n",
    "# Step 5: Fine-tune the model\n",
    "# Create InputExamples\n",
    "train_examples = [\n",
    "    InputExample(texts=[s1, s2], label=score)\n",
    "    for s1, s2, score in zip(train_sentences1, train_sentences2, train_scores)\n",
    "]\n",
    "\n",
    "# Create a DataLoader\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "\n",
    "# Define the loss function\n",
    "train_loss = losses.CosineSimilarityLoss(original_model)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(message)s',\n",
    "    level=logging.INFO,\n",
    "    handlers=[LoggingHandler()]\n",
    ")\n",
    "\n",
    "# Fine-tune the model with progress bar and logging\n",
    "print(\"Fine-tuning the SBERT model...\")\n",
    "original_model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=10,\n",
    "    warmup_steps=100,\n",
    "    show_progress_bar=True,\n",
    "    output_path=fine_tuned_model_path\n",
    ")\n",
    "print(f\"Fine-tuned model was saved at {fine_tuned_model_path}\")\n",
    "\n",
    "# Step 6: Evaluate the fine-tuned model\n",
    "print(\"Evaluating the fine-tuned SBERT model...\")\n",
    "fine_tuned_model = SentenceTransformer(fine_tuned_model_path)\n",
    "fine_tuned_evaluator = EmbeddingSimilarityEvaluator(val_sentences1, val_sentences2, val_scores)\n",
    "fine_tuned_score = fine_tuned_evaluator(fine_tuned_model)\n",
    "print_results(fine_tuned_score, \"Fine-tuned SBERT model\")\n",
    "\n",
    "# Step 7: Compare results\n",
    "print(\"\\nEvaluation Comparison:\")\n",
    "print_results(evaluation_results, \"Original SBERT model\")\n",
    "print_results(fine_tuned_score, \"Fine-tuned SBERT model\")\n"
   ],
   "id": "2fd510965342c2bb",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stefan\\Documents\\GitHub\\UBB-Y3-AI-Research\\models\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "GPU: NVIDIA GeForce GTX 1080 Ti\n",
      "Loading the STS-B dataset...\n",
      "Limiting dataset to 4000 rows...\n",
      "Normalizing scores...\n",
      "Loading the original SBERT model...\n",
      "Evaluating the original SBERT model...\n",
      "SBERT model pearson_cosine: 0.8796\n",
      "SBERT model spearman_cosine: 0.8816\n",
      "Fine-tuning the SBERT model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/2500 19:01, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.006600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.004700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.002400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.001600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.001200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-14 23:54:17,975 - Save model to fine_tuned_sbert_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model was saved at fine_tuned_sbert_model\n",
      "Evaluating the fine-tuned SBERT model...\n",
      "2025-01-14 23:54:21,286 - Use pytorch device_name: cuda\n",
      "2025-01-14 23:54:21,286 - Load pretrained SentenceTransformer: fine_tuned_sbert_model\n",
      "2025-01-14 23:54:22,775 - EmbeddingSimilarityEvaluator: Evaluating the model on the  dataset:\n",
      "2025-01-14 23:54:31,649 - Cosine-Similarity :\tPearson: 0.8801\tSpearman: 0.8816\n",
      "Fine-tuned SBERT model pearson_cosine: 0.8801\n",
      "Fine-tuned SBERT model spearman_cosine: 0.8816\n",
      "\n",
      "Evaluation Comparison:\n",
      "Original SBERT model pearson_cosine: 0.8796\n",
      "Original SBERT model spearman_cosine: 0.8816\n",
      "Fine-tuned SBERT model pearson_cosine: 0.8801\n",
      "Fine-tuned SBERT model spearman_cosine: 0.8816\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
