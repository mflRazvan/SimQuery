{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence_transformers in ./.local/lib/python3.10/site-packages (3.3.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./.local/lib/python3.10/site-packages (from sentence_transformers) (4.41.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./.local/lib/python3.10/site-packages (from sentence_transformers) (1.11.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./.local/lib/python3.10/site-packages (from sentence_transformers) (0.23.2)\n",
      "Requirement already satisfied: Pillow in /usr/lib/python3/dist-packages (from sentence_transformers) (9.0.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (5.4.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.10.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.local/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.2.2)\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3 -m pip install --upgrade pip\u001B[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (23.2)\n",
      "Downloading faiss_cpu-1.9.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m27.5/27.5 MB\u001B[0m \u001B[31m8.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0mm\n",
      "\u001B[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.9.0.post1\n",
      "\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.0\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpython3 -m pip install --upgrade pip\u001B[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence_transformers\n",
    "%pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T10:10:54.196719Z",
     "start_time": "2024-12-19T10:10:50.273672Z"
    }
   },
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Check if MPS is available and set the device accordingly\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Model pentru embeddings\n",
    "model = SentenceTransformer('all-mpnet-base-v2', device=device)\n",
    "# model = SentenceTransformer('fine_tuned_model')\n",
    "\n",
    "# Text căutat\n",
    "# query = \"Impactul schimbărilor climatice asupra biodiversității.\"\n",
    "#\n",
    "# # Ground truth (listează segmentele documentului)\n",
    "# documents = [\n",
    "#     \"Schimbările climatice cauzează o pierdere masivă a biodiversității la nivel global.\",\n",
    "#     \"Tehnologiile verzi pot ajuta la reducerea emisiilor de CO2.\",\n",
    "#     \"Biodiversitatea este crucială pentru ecosistemele sănătoase.\"\n",
    "# ]\n",
    "\n",
    "query = \"Donald Trump recently visited the UK.\"\n",
    "documents = [\n",
    "    \"The President of the United States visited the UK.\",\n",
    "    \"The UK was visited by the President of the United States.\",\n",
    "    \"ONU congress decides to ban nuclear weapons.\",\n",
    "    \"The sky is blue.\",\n",
    "    \"The sun is shining.\",\n",
    "    \"Recipe for a delicious cake.\"\n",
    "]\n",
    "\n",
    "# Generare embeddings pentru ground truth\n",
    "doc_embeddings = model.encode(documents)\n",
    "\n",
    "# Generare embeddings\n",
    "query_embedding = model.encode([query])\n",
    "\n",
    "# Normalizează embeddings\n",
    "doc_embeddings = doc_embeddings / np.linalg.norm(doc_embeddings, axis=1, keepdims=True)\n",
    "query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)\n",
    "\n",
    "# Creare index FAISS\n",
    "dimension = doc_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(np.array(doc_embeddings))\n",
    "\n",
    "# Căutare în index\n",
    "k = len(documents)\n",
    "distances, indices = index.search(np.array(query_embedding), k)\n",
    "\n",
    "# Afișare rezultate\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    fragment = documents[idx]\n",
    "    score = distances[0][i] \n",
    "    print(f\"Fragment: {fragment}\\nGrad de similitudine: {score:.2f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fragment: The President of the United States visited the UK.\n",
      "Grad de similitudine: 0.79\n",
      "Fragment: The UK was visited by the President of the United States.\n",
      "Grad de similitudine: 0.70\n",
      "Fragment: The sun is shining.\n",
      "Grad de similitudine: 0.10\n",
      "Fragment: The sky is blue.\n",
      "Grad de similitudine: 0.09\n",
      "Fragment: ONU congress decides to ban nuclear weapons.\n",
      "Grad de similitudine: 0.06\n",
      "Fragment: Recipe for a delicious cake.\n",
      "Grad de similitudine: 0.02\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
