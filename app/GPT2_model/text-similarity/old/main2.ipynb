{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Text Similarity with GPT-2 (version 2)",
   "id": "a72494e30a765261"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Fine-tuning GPT-2 for Text Similarity",
   "id": "f61c723babf06850"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(device)"
   ],
   "id": "dad4290bf573cc16",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification, Trainer, TrainingArguments, GPT2LMHeadModel\n",
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
    "import random\n",
    "\n",
    "# Ratio for limiting the number of examples\n",
    "ratio = 0.05\n",
    "load_local_model = False\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the readerbench dataset\n",
    "dataset = load_dataset(\"readerbench/ro_fake_news\", \"eda\")\n",
    "\n",
    "considered_tag_words = ['5g', 'vaccinare', 'controlul']\n",
    "\n",
    "# Function to create labeled pairs with both similar and dissimilar examples\n",
    "def preprocess_data(example, all_bodies=None):\n",
    "    example['text'] = example['headline'] + \" \" + example['body']\n",
    "\n",
    "    # Label the pair as similar\n",
    "    example['label'] = 1.0\n",
    "\n",
    "    # Add dissimilar pairs if bodies list is available\n",
    "    dissimilar_examples = []\n",
    "    if all_bodies:\n",
    "        for other_body in random.sample(all_bodies, min(5, len(all_bodies))):  # Pick a few random other bodies\n",
    "            dissimilar_example = {\n",
    "                'text': example['headline'] + \" \" + other_body,\n",
    "                'label': 0.0\n",
    "            }\n",
    "            dissimilar_examples.append(dissimilar_example)\n",
    "    return [example] + dissimilar_examples\n",
    "\n",
    "\n",
    "# Collect all bodies from the dataset to use for dissimilar examples\n",
    "all_bodies = [item['body'] for subset in dataset.values() for item in subset]\n",
    "\n",
    "# Limit the number of bodies to a ratio of the total number of examples\n",
    "limit = ratio * sum([len(subset) for subset in dataset.values()])   \n",
    "\n",
    "print(f\"Processing {int(limit)} similar examples.\")\n",
    "\n",
    "# Apply the function to each instance in the dataset and flatten the results\n",
    "processed_data = []\n",
    "i = 0\n",
    "for subset in dataset.values():\n",
    "    for example in subset:\n",
    "        processed_data.extend(preprocess_data(example, all_bodies=all_bodies))\n",
    "        \n",
    "        i += 1\n",
    "        if i >= limit:\n",
    "            break\n",
    "    if i >= limit:\n",
    "        break\n",
    "        \n",
    "print(f\"Processed {len(processed_data)} examples.\")\n",
    "\n",
    "# Convert processed data to DataFrame\n",
    "processed_df = pd.DataFrame(processed_data)\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "train_df = processed_df[:int(len(processed_df) * 0.9)]\n",
    "validation_df = processed_df[int(len(processed_df) * 0.9):]\n",
    "\n",
    "# Convert the DataFrames to Dataset objects\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "validation_dataset = Dataset.from_pandas(validation_df)\n",
    "\n",
    "# Create a DatasetDict for train and validation\n",
    "combined_dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset\n",
    "})\n",
    "\n",
    "# Load the GPT-2 tokenizer and model for sequence classification\n",
    "if load_local_model:\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-romanian\")\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"./gpt2-romanian\")\n",
    "else:\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=1)\n",
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {trainable_params}\")\n",
    "\n",
    "# Add a pad token to the tokenizer and model\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Move the model to the GPU\n",
    "model.to(device)\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_data(example):\n",
    "    encoding = tokenizer(\n",
    "        example['text'],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "    )\n",
    "    encoding['labels'] = example['label']\n",
    "    return encoding\n",
    "\n",
    "\n",
    "# Apply tokenization\n",
    "train_data = combined_dataset[\"train\"].map(tokenize_data)\n",
    "eval_data = combined_dataset[\"validation\"].map(tokenize_data)\n",
    "\n",
    "train_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "eval_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../gpt2-similarity\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained(\"./gpt2-similarity\")\n",
    "tokenizer.save_pretrained(\"./gpt2-similarity\")"
   ],
   "id": "a6766d23b9659618",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Loading and Testing the Fine-Tuned Model",
   "id": "b978f3625e8fc953"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"../gpt2-similarity\")\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"../gpt2-similarity\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Define a function to calculate similarity\n",
    "def calculate_similarity(prompt, ground_truth_text):\n",
    "    inputs = tokenizer(prompt + \" \" + ground_truth_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "    inputs = {key: value.to(device) for key, value in inputs.items()}  # Move inputs to GPU\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        similarity_score = torch.sigmoid(outputs.logits).item()  # Convert logits to probability\n",
    "    return similarity_score\n",
    "\n",
    "def test_similarity(prompt, ground_truth_text):\n",
    "    similarity = calculate_similarity(prompt, ground_truth_text)\n",
    "    print(f\"Similarity score between prompt and ground truth: {similarity:.2f}\")\n",
    "\n",
    "# Test the model with a custom prompt and ground truth\n",
    "prompt1 = \"Covid-19 are legatura cu reteaua 5G, conform unor teorii ale conspiratiei.\"\n",
    "prompt2 = \"Vaccinarea este o conspiratie.\"\n",
    "prompt3 = \"Politica este implicata in controlul populatiei.\"\n",
    "ground_truth_text = \"In Regatul Unit se răspândește un val de teorii ale conspirației despre o potentiala legătură între rețeaua 5G și răspândirea virusului COVID-19. Aceste teorii au fost deja demontate de experți, însă mulți oameni continuă să le creadă.\"\n",
    "\n",
    "test_similarity(prompt1, ground_truth_text)\n",
    "test_similarity(prompt2, ground_truth_text)\n",
    "test_similarity(prompt3, ground_truth_text)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Checking for GPU Availability",
   "id": "7d4e8ffc84ddfef8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True if a GPU is available\n",
    "print(torch.cuda.device_count())  # Number of GPUs available\n",
    "print(torch.cuda.current_device())  # Index of the current GPU"
   ],
   "id": "9c2640f45eeb148c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
